<?xml version="1.0"?>
<Container version="2">
  <Name>LocalDeepResearch</Name>
  <Repository>localdeepresearch/local-deep-research:latest</Repository>
  <Registry>https://hub.docker.com/r/localdeepresearch/local-deep-research</Registry>
  <Network>bridge</Network>
  <Shell>sh</Shell>
  <Privileged>false</Privileged>
  <Support>https://github.com/LearningCircuit/local-deep-research/issues</Support>
  <Project>https://github.com/LearningCircuit/local-deep-research</Project>
  <Overview>Local Deep Research (LDR) is an AI-powered research assistant that performs systematic research by breaking down complex questions, searching multiple sources in parallel, verifying information across sources, and creating comprehensive reports with proper citations. It runs entirely locally with Ollama + SearXNG for privacy, or can use cloud LLM providers.&#xD;
&#xD;
Features:&#xD;
- Privacy-focused: Run entirely locally with Ollama&#xD;
- Multiple research modes (Quick Summary, Detailed Research)&#xD;
- Per-user encrypted databases (SQLCipher with AES-256)&#xD;
- Real-time progress tracking via WebSocket&#xD;
- Export to PDF or Markdown&#xD;
- REST API with authentication&#xD;
- Analytics dashboard&#xD;
&#xD;
This template includes the main LDR service. For full functionality, you may also want to install the companion containers (Ollama and SearXNG) or configure external LLM/search providers.&#xD;
&#xD;
[b]Community &amp; Support:[/b]&#xD;
- Discord: https://discord.gg/ttcqQeFcJ3&#xD;
- Reddit: https://www.reddit.com/r/LocalDeepResearch/&#xD;
- GitHub Issues: https://github.com/LearningCircuit/local-deep-research/issues</Overview>
  <Category>AI: Productivity:</Category>
  <WebUI>http://[IP]:[PORT:5000]</WebUI>
  <TemplateURL>https://raw.githubusercontent.com/selfhosters/unRAID-CA-templates/master/templates/local-deep-research.xml</TemplateURL>
  <Icon>https://raw.githubusercontent.com/LearningCircuit/local-deep-research/main/src/local_deep_research/web/static/favicon.png</Icon>
  <DonateText>Support this project with a GitHub star!</DonateText>
  <DonateLink>https://github.com/LearningCircuit/local-deep-research</DonateLink>
  <Requires>Optional: Ollama container for local LLM, SearXNG container for local search</Requires>
  <Config Name="WebUI Port" Target="5000" Default="5000" Mode="tcp" Description="Port for accessing the Local Deep Research web interface. Change the host port (left side) if 5000 is already in use. Do NOT change the container port (right side)." Type="Port" Display="always" Required="true" Mask="false">5000</Config>
  <Config Name="Data Directory" Target="/data" Default="/mnt/user/appdata/local-deep-research/data" Mode="rw" Description="Primary data directory containing user databases, research outputs, cache, and logs. All user data is stored here." Type="Path" Display="always" Required="true" Mask="false">/mnt/user/appdata/local-deep-research/data</Config>
  <Config Name="Scripts Directory" Target="/scripts" Default="/mnt/user/appdata/local-deep-research/scripts" Mode="rw" Description="Directory for startup scripts. Required for multi-container setups with Ollama." Type="Path" Display="always" Required="true" Mask="false">/mnt/user/appdata/local-deep-research/scripts</Config>
  <Config Name="Personal Notes (Optional)" Target="/local_collections/personal_notes" Default="" Mode="ro" Description="Optional: Mount a directory containing personal notes/documents to search. Leave empty if not needed." Type="Path" Display="advanced" Required="false" Mask="false"></Config>
  <Config Name="Project Docs (Optional)" Target="/local_collections/project_docs" Default="" Mode="ro" Description="Optional: Mount a directory containing project documentation to search. Leave empty if not needed." Type="Path" Display="advanced" Required="false" Mask="false"></Config>
  <Config Name="Research Papers (Optional)" Target="/local_collections/research_papers" Default="" Mode="ro" Description="Optional: Mount a directory containing research papers to search. Leave empty if not needed." Type="Path" Display="advanced" Required="false" Mask="false"></Config>
  <Config Name="Web Host" Target="LDR_WEB_HOST" Default="0.0.0.0" Mode="" Description="Web server host binding. MUST be 0.0.0.0 for Docker. Do NOT change this." Type="Variable" Display="advanced" Required="true" Mask="false">0.0.0.0</Config>
  <Config Name="Web Port (Internal)" Target="LDR_WEB_PORT" Default="5000" Mode="" Description="Internal container port. MUST be 5000. Do NOT change this. To change the external port, modify the Port mapping above." Type="Variable" Display="advanced" Required="true" Mask="false">5000</Config>
  <Config Name="Data Directory (Internal)" Target="LDR_DATA_DIR" Default="/data" Mode="" Description="Internal data directory path. MUST be /data. Do NOT change this." Type="Variable" Display="advanced" Required="true" Mask="false">/data</Config>
  <Config Name="Ollama URL" Target="LDR_LLM_OLLAMA_URL" Default="http://ollama:11434" Mode="" Description="Ollama API URL. Use 'http://ollama:11434' if running Ollama container on ldr-network, or 'http://[IP]:11434' for external Ollama instance." Type="Variable" Display="always" Required="false" Mask="false">http://ollama:11434</Config>
  <Config Name="SearXNG URL" Target="LDR_SEARCH_ENGINE_WEB_SEARXNG_DEFAULT_PARAMS_INSTANCE_URL" Default="http://searxng:8080" Mode="" Description="SearXNG instance URL. Use 'http://searxng:8080' if running SearXNG container on ldr-network, or configure external search engine in WebUI." Type="Variable" Display="always" Required="false" Mask="false">http://searxng:8080</Config>
  <Config Name="LLM Provider" Target="LDR_LLM_PROVIDER" Default="" Mode="" Description="Optional: Force LLM provider (ollama, openai, anthropic, google). Leave empty to configure via WebUI Settings page. Only set this to LOCK the provider." Type="Variable" Display="advanced" Required="false" Mask="false"></Config>
  <Config Name="LLM Model" Target="LDR_LLM_MODEL" Default="" Mode="" Description="Optional: Force specific model name. Leave empty to configure via WebUI Settings page. Only set this to LOCK the model." Type="Variable" Display="advanced" Required="false" Mask="false"></Config>
  <Config Name="OpenAI API Key" Target="LDR_LLM_OPENAI_API_KEY" Default="" Mode="" Description="Optional: OpenAI API key. Can also be configured via WebUI Settings page. Only set here to LOCK it (prevents UI changes)." Type="Variable" Display="advanced" Required="false" Mask="true"></Config>
  <Config Name="Anthropic API Key" Target="LDR_LLM_ANTHROPIC_API_KEY" Default="" Mode="" Description="Optional: Anthropic API key. Can also be configured via WebUI Settings page. Only set here to LOCK it (prevents UI changes)." Type="Variable" Display="advanced" Required="false" Mask="true"></Config>
  <Config Name="Google API Key" Target="LDR_LLM_GOOGLE_API_KEY" Default="" Mode="" Description="Optional: Google API key. Can also be configured via WebUI Settings page. Only set here to LOCK it (prevents UI changes)." Type="Variable" Display="advanced" Required="false" Mask="true"></Config>
  <Config Name="PUID" Target="PUID" Default="99" Mode="" Description="User ID for file permissions. Default 99 (nobody) is recommended for Unraid." Type="Variable" Display="advanced" Required="false" Mask="false">99</Config>
  <Config Name="PGID" Target="PGID" Default="100" Mode="" Description="Group ID for file permissions. Default 100 (users) is recommended for Unraid." Type="Variable" Display="advanced" Required="false" Mask="false">100</Config>
</Container>
